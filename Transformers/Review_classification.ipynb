{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29ce689-a0b6-4570-83c3-92a103d97e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071ac03-e828-41fa-b948-9ff51fec6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257b39f-a013-46c2-a7ef-d25461e1acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f7218-8d8a-40f3-84b4-c62a2450bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df.sentiment.map(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457dd6cf-39ee-4e13-bf1e-42123ebf8df3",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114239b-45b1-4bfc-8678-aa1440c3ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['review'].iloc[idx]\n",
    "        label = self.df['sentiment'].iloc[idx]\n",
    "        \n",
    "        tokenized_text = self.tokenizer.encode_plus(\n",
    "            text=text, \n",
    "            max_length=128, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized_text['input_ids']\n",
    "        attention_mask = tokenized_text['attention_mask']\n",
    "        token_type_ids = tokenized_text['token_type_ids']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd1071-7d6b-491c-bf86-8589ff0a0e8f",
   "metadata": {},
   "source": [
    "## The DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f0f0f-21ac-495d-b1e8-aa7f59296c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.dataset = ImdbDataset(df)\n",
    "\n",
    "    def setup(self, stage) -> None:\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            lengths = [\n",
    "                int(len(self.dataset) * 0.8), \n",
    "                int(len(self.dataset) * 0.2)\n",
    "            ]\n",
    "            self.train_data, self.val_data = random_split(self.dataset, lengths)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=8, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=8, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170aeae-c40e-4573-9ef7-854d2f917126",
   "metadata": {},
   "source": [
    "## The LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b956fc5-f43b-4ef8-9432-bd2c1c9685d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitImdb(pl.LightningModule):\n",
    "    def __init__(self, fine_tune=True):\n",
    "        super(LitImdb, self).__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        self.model.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=1\n",
    "        )\n",
    "        if fine_tune:\n",
    "            self.freeze()\n",
    "        \n",
    "    def freeze(self):\n",
    "        for param in self.model.named_parameters():\n",
    "            if 'classifer' not in param[0]:\n",
    "                param[1].requires_grad = False\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.model.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_masks = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        targets = batch['label']\n",
    "\n",
    "        preds = self.forward(\n",
    "            input_ids, attention_masks, token_type_ids\n",
    "        )\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(input=preds, target=targets)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_masks = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        targets = batch['label']\n",
    "\n",
    "        preds = self.forward(\n",
    "            input_ids, attention_masks, token_type_ids\n",
    "        )\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(input=preds, target=targets)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed696ba0-d8c7-498c-8171-c686e589311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitImdb()\n",
    "dm = ImdbDataModule(df)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=True,\n",
    "    checkpoint_callback=True,\n",
    "    gpus=1,\n",
    "    max_epochs=3,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef838fb-5a30-437d-bf9b-5329982941c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
